diff --git a/include/trace/events/mptcp.h b/include/trace/events/mptcp.h
index 085b749cdd97..f31681a1f479 100644
--- a/include/trace/events/mptcp.h
+++ b/include/trace/events/mptcp.h
@@ -62,6 +62,53 @@ TRACE_EVENT(mptcp_subflow_get_send,
 		  __entry->backup, __entry->ratio)
 );
 
+TRACE_EVENT(mptcp_subflow_get_retrans,
+
+	TP_PROTO(struct mptcp_subflow_context *subflow),
+
+	TP_ARGS(subflow),
+
+	TP_STRUCT__entry(
+		__field(bool, active)
+		__field(bool, free)
+		__field(u32, snd_wnd)
+		__field(u32, pace)
+		__field(u8, backup)
+		__field(u64, ratio)
+	),
+
+	TP_fast_assign(
+		struct sock *ssk;
+
+		__entry->active = mptcp_subflow_active(subflow);
+		__entry->backup = subflow->backup;
+
+		if (subflow->tcp_sock && sk_fullsock(subflow->tcp_sock))
+			__entry->free = sk_stream_memory_free(subflow->tcp_sock);
+		else
+			__entry->free = 0;
+
+		ssk = mptcp_subflow_tcp_sock(subflow);
+		if (ssk && sk_fullsock(ssk)) {
+			__entry->snd_wnd = tcp_sk(ssk)->snd_wnd;
+			__entry->pace = READ_ONCE(ssk->sk_pacing_rate);
+		} else {
+			__entry->snd_wnd = 0;
+			__entry->pace = 0;
+		}
+
+		if (ssk && sk_fullsock(ssk) && __entry->pace)
+			__entry->ratio = div_u64((u64)ssk->sk_wmem_queued << 32, __entry->pace);
+		else
+			__entry->ratio = 0;
+	),
+
+	TP_printk("active=%d free=%d snd_wnd=%u pace=%u backup=%u ratio=%llu",
+		  __entry->active, __entry->free,
+		  __entry->snd_wnd, __entry->pace,
+		  __entry->backup, __entry->ratio)
+);
+
 DECLARE_EVENT_CLASS(mptcp_dump_mpext,
 
 	TP_PROTO(struct mptcp_ext *mpext),
diff --git a/net/mptcp/protocol.c b/net/mptcp/protocol.c
index c2317919fc14..41ae28d27de5 100644
--- a/net/mptcp/protocol.c
+++ b/net/mptcp/protocol.c
@@ -2314,6 +2314,7 @@ struct sock *mptcp_subflow_get_retrans(struct mptcp_sock *msk)
 	int min_stale_count = INT_MAX;
 
 	mptcp_for_each_subflow(msk, subflow) {
+		trace_mptcp_subflow_get_retrans(subflow);
 		struct sock *ssk = mptcp_subflow_tcp_sock(subflow);
 
 		if (!__mptcp_subflow_active(subflow))
@@ -2343,6 +2344,62 @@ struct sock *mptcp_subflow_get_retrans(struct mptcp_sock *msk)
 	return min_stale_count > 1 ? backup : NULL;
 }
 
+struct sock *mptcp_subflow_get_roundrobin(struct mptcp_sock *msk)
+{
+	struct subflow_send_info send_info[SSK_MODE_MAX];
+	struct mptcp_subflow_context *subflow;
+	struct sock *sk = (struct sock *)msk;
+	u32 burst;
+	int i = 0;
+	struct sock *ssk;
+	long tout = 0;
+
+	for (i = 0; i < SSK_MODE_MAX; ++i) {
+		send_info[i].ssk = NULL;
+		send_info[i].linger_time = -1;
+	}
+
+	mptcp_for_each_subflow(msk, subflow) {
+		trace_mptcp_subflow_get_send(subflow);
+		ssk =  mptcp_subflow_tcp_sock(subflow);
+		if (!mptcp_subflow_active(subflow))
+			continue;
+
+		if (!send_info[SSK_MODE_BACKUP].ssk)
+			send_info[SSK_MODE_BACKUP].ssk = ssk;
+		if (subflow->already_chosen)
+			continue;
+
+		send_info[SSK_MODE_ACTIVE].ssk = ssk;
+		break;
+	}
+	__mptcp_set_timeout(sk, tout);
+
+	// If all subflows were set as already chosen,
+	// take the first subflow, and reset all subflows status
+	if (!send_info[SSK_MODE_ACTIVE].ssk) {
+		send_info[SSK_MODE_ACTIVE].ssk = send_info[SSK_MODE_BACKUP].ssk;
+
+		mptcp_for_each_subflow(msk, subflow) {
+			subflow->already_chosen = false;
+		}
+	}
+
+	ssk = send_info[SSK_MODE_ACTIVE].ssk;
+	if (!ssk || !sk_stream_memory_free(ssk))
+		return NULL;
+
+	subflow = mptcp_subflow_ctx(ssk);
+	subflow->already_chosen = true;
+
+	burst = min_t(int, MPTCP_SEND_BURST_SIZE, mptcp_wnd_end(msk) - msk->snd_nxt);
+	if (!burst)
+		return ssk;
+
+	msk->snd_burst = burst;
+	return ssk;
+}
+
 bool __mptcp_retransmit_pending_data(struct sock *sk)
 {
 	struct mptcp_data_frag *cur, *rtx_head;
diff --git a/net/mptcp/protocol.h b/net/mptcp/protocol.h
index 74417aae08d0..f1e067f8f15c 100644
--- a/net/mptcp/protocol.h
+++ b/net/mptcp/protocol.h
@@ -535,6 +535,7 @@ struct mptcp_subflow_context {
 		__unused : 8;
 	bool	data_avail;
 	bool	scheduled;
+	bool	already_chosen;
 	u32	remote_nonce;
 	u64	thmac;
 	u32	local_nonce;
@@ -749,6 +750,7 @@ void mptcp_subflow_set_scheduled(struct mptcp_subflow_context *subflow,
 				 bool scheduled);
 struct sock *mptcp_subflow_get_send(struct mptcp_sock *msk);
 struct sock *mptcp_subflow_get_retrans(struct mptcp_sock *msk);
+struct sock *mptcp_subflow_get_roundrobin(struct mptcp_sock *msk);
 int mptcp_sched_get_send(struct mptcp_sock *msk);
 int mptcp_sched_get_retrans(struct mptcp_sock *msk);
 
diff --git a/net/mptcp/sched.c b/net/mptcp/sched.c
index 78ed508ebc1b..db79fb5d48a3 100644
--- a/net/mptcp/sched.c
+++ b/net/mptcp/sched.c
@@ -30,12 +30,33 @@ static int mptcp_sched_default_get_subflow(struct mptcp_sock *msk,
 	return 0;
 }
 
+static int mptcp_sched_roundrobin_get_subflow(struct mptcp_sock *msk,
+					   struct mptcp_sched_data *data)
+{
+	struct sock *ssk;
+
+	ssk = data->reinject ? mptcp_subflow_get_retrans(msk) :
+			       mptcp_subflow_get_roundrobin(msk);
+
+	if (!ssk)
+		return -EINVAL;
+
+	mptcp_subflow_set_scheduled(mptcp_subflow_ctx(ssk), true);
+	return 0;
+}
+
 static struct mptcp_sched_ops mptcp_sched_default = {
 	.get_subflow	= mptcp_sched_default_get_subflow,
 	.name		= "default",
 	.owner		= THIS_MODULE,
 };
 
+static struct mptcp_sched_ops mptcp_sched_roundrobin = {
+	.get_subflow	= mptcp_sched_roundrobin_get_subflow,
+	.name		= "roundrobin",
+	.owner		= THIS_MODULE,
+};
+
 /* Must be called with rcu read lock held */
 struct mptcp_sched_ops *mptcp_sched_find(const char *name)
 {
@@ -103,6 +124,7 @@ void mptcp_unregister_scheduler(struct mptcp_sched_ops *sched)
 void mptcp_sched_init(void)
 {
 	mptcp_register_scheduler(&mptcp_sched_default);
+	mptcp_register_scheduler(&mptcp_sched_roundrobin);
 }
 
 int mptcp_init_sched(struct mptcp_sock *msk,
